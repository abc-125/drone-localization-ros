#!/usr/bin/env python

# based on https://github.com/DavidFernandezChaves/Detectron2_ros

import sys
import threading
import time

import cv2 as cv
import numpy as np
import rospy
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
from cv_bridge import CvBridge
from image_geometry import PinholeCameraModel

from detectron2.utils.logger import setup_logger
from detectron2.utils.visualizer import Visualizer
from drone_localization_ros.msg import Detection, Detections, PosesWithCovariance
from sensor_msgs.msg import Image, CameraInfo
from geometry_msgs.msg import PoseWithCovariance

from predictorNoResize import PredictorNoResize



class DroneLocalizationNode(object):
    def __init__(self):
        rospy.logwarn("Initializing")
        setup_logger()

        self._bridge = CvBridge()
        self._last_msg = None
        self._msg_lock = threading.Lock()
        self._image_counter = 0

        self.camera = PinholeCameraModel()
        self._camera_info = None
        self._drone_width = self.load_param('~drone_width')
        self.covariance_xy = 0.05
        self.covariance_z = 0.5

        self.mode = self.load_param('~network_size')  # small / large / dual

        self.small_input_size = (320, 256)
        self.large_input_size = (880, 680)

        # either small or large neural network
        if self.mode != "dual":
            self.cfg1 = get_cfg()

            if self.mode == "small":
                self.cfg1.merge_from_file(self.load_param('~config_small'))
                self.cfg1.MODEL.WEIGHTS = self.load_param('~model_small')
                self.cfg1.MODEL.ANCHOR_GENERATOR.SIZES = [[8, 16, 32, 64]]

            if self.mode == "large":
                self.cfg1.merge_from_file(self.load_param('~config_large'))
                self.cfg1.MODEL.WEIGHTS = self.load_param('~model_large')
                self.cfg1.MODEL.ANCHOR_GENERATOR.SIZES = [[16, 32, 64, 128, 256]]

            self.cfg1.MODEL.ROI_HEADS.SCORE_THRESH_TEST = self.load_param('~detection_threshold')
            self.cfg1.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.5, 1.0]]
            self.cfg1.MODEL.ROI_HEADS.NUM_CLASSES = 1 
            self.cfg1.MODEL.DEVICE='cpu'  # for test and compatibility

            self.predictor = PredictorNoResize(self.cfg1)

            MetadataCatalog.get(self.cfg1.DATASETS.TRAIN[0]).set(thing_classes=["drone"])  # set class name
            self._class_names = MetadataCatalog.get(self.cfg1.DATASETS.TRAIN[0]).get("thing_classes", None)
    
        # both neural networks
        if self.mode == "dual":
            self.no_detections_treshold = self.load_param('~no_detections_treshold')

            self.cfg1 = get_cfg()
            self.cfg1.merge_from_file(self.load_param('~config_small'))
            self.cfg1.MODEL.WEIGHTS = self.load_param('~model_small')
            self.cfg1.MODEL.ANCHOR_GENERATOR.SIZES = [[8, 16, 32, 64]]

            self.cfg2 = get_cfg()
            self.cfg2.merge_from_file(self.load_param('~config_large'))
            self.cfg2.MODEL.WEIGHTS = self.load_param('~model_large')
            self.cfg2.MODEL.ANCHOR_GENERATOR.SIZES = [[16, 32, 64, 128, 256]]
            
            self.cfg1.MODEL.ROI_HEADS.SCORE_THRESH_TEST = self.cfg2.MODEL.ROI_HEADS.SCORE_THRESH_TEST = self.load_param('~detection_threshold')
            self.cfg1.MODEL.ROI_HEADS.NUM_CLASSES = self.cfg2.MODEL.ROI_HEADS.NUM_CLASSES = 1 
            self.cfg1.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = self.cfg2.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.5, 1.0]]
            self.cfg1.MODEL.DEVICE = self.cfg2.MODEL.DEVICE = 'cpu'  # for test and compatibility

            self.predictor1 = PredictorNoResize(self.cfg1)
            self.predictor2 = PredictorNoResize(self.cfg2)

            MetadataCatalog.get(self.cfg1.DATASETS.TRAIN[0]).set(thing_classes=["drone"])  # set class name
            self._class_names = MetadataCatalog.get(self.cfg1.DATASETS.TRAIN[0]).get("thing_classes", None)

        self._visualization = self.load_param('~visualization', True)
        self._detections_pub = rospy.Publisher('~detections', Detections, queue_size=1)
        self._poses_pub = rospy.Publisher('~poses', PosesWithCovariance, queue_size=1)
        self._vis_pub = rospy.Publisher('~visualization', Image, queue_size=1)
        self._sub = rospy.Subscriber(self.load_param('~input_images'), Image, self.callback_image, queue_size=1)
        self._sub_cam_info = rospy.Subscriber(self.load_param('~camera_info'), CameraInfo, self.callback_cam_info, queue_size=1)
        self.start_time = time.time()
        rospy.logwarn("Initialized")


    def run(self):
        rate = rospy.Rate(100)
        detections_counter = 0

        while not rospy.is_shutdown():
            if self._msg_lock.acquire(False):
                img_msg = self._last_msg
                self._last_msg = None
                self._msg_lock.release()
            else:
                rate.sleep()
                continue

            if img_msg and self._camera_info:
                np_image = self.convert_to_cv_image(img_msg)
                
                # small or large mode (one neural network)
                if self.mode != "dual":
                    if self.mode == "small":
                        np_image = cv.resize(np_image, self.small_input_size)
                    if self.mode == "large":
                        np_image = cv.resize(np_image, self.large_input_size)

                    outputs = self.predictor(np_image)
                    detections = outputs["instances"].to("cpu")

                # dual mode (two neural networks)
                else:
                    if detections_counter < self.no_detections_treshold:
                        np_image = cv.resize(np_image, self.small_input_size)
                        outputs = self.predictor1(np_image)  # use faster network
                    else:
                        rospy.loginfo("Cannot detect target, using larger neural network for detection...")
                        np_image = cv.resize(np_image, self.large_input_size)
                        outputs = self.predictor2(np_image)  # use slower network if no detections for some number of images
                    detections = outputs["instances"].to("cpu")    

                    if not detections:
                        detections_counter += 1
                    else:
                        detections_counter = 0

                # publish detections
                dets_msg = self.get_detections_msg(detections)
                self._detections_pub.publish(dets_msg)
                #rospy.loginfo(detections_msg)

                # calculate relative drone localization
                poses_msg = PosesWithCovariance()
                for det in dets_msg.detections:
                    x_mid = det.xmin + (det.xmax - det.xmin)/2.0
                    y_mid = det.ymin + (det.ymax - det.ymin)/2.0
                    center_vector = np.array(self.camera.projectPixelTo3dRay((x_mid, y_mid)))  # self.camera.rectifyPoint TODO?
                    right_vector = np.array(self.camera.projectPixelTo3dRay((det.xmax, y_mid)))

                    estimated_distance = self._drone_width / 2.0 * np.linalg.norm(center_vector) \
                        / np.linalg.norm(right_vector - center_vector)

                    if estimated_distance > 0 and estimated_distance is not np.NaN:
                        print("estimated_distance: " + str(estimated_distance))  # TODO
                        position_vector = estimated_distance * center_vector
                        pose_w_cov_msg = PoseWithCovariance()
                        pose_w_cov_msg.pose.position.x = position_vector[0]
                        pose_w_cov_msg.pose.position.y = position_vector[1]
                        pose_w_cov_msg.pose.position.z = position_vector[2]
                        pose_w_cov_msg.pose.orientation.x = 0
                        pose_w_cov_msg.pose.orientation.y = 0
                        pose_w_cov_msg.pose.orientation.z = 0
                        pose_w_cov_msg.pose.orientation.w = 1

                        # calculate covariance
                        cov_matrix = np.identity(3)
                        cov_matrix[0][0] = cov_matrix[1][1] = self.covariance_xy
                        cov_matrix[2][2] = position_vector[2] * np.sqrt(position_vector[2]) * self.covariance_z
                        if cov_matrix[2][2] < 0.33 * self.covariance_z:
                            cov_matrix[2][2] = 0.33 * self.covariance_z
                        # rotation
                        rotation_vector = self.rotation_matrix_from_vectors(np.array([0.0, 0.0, 1.0]), position_vector)
                        rotated_cov = rotation_vector * cov_matrix * np.transpose(rotation_vector)
                        # fill cov matrix
                        cov = [0.0] * 36
                        for r in range(6):
                            for c in range(6):
                                if r < 3 and c < 3:
                                    cov[r * 6 + c] = rotated_cov[r][c]
                                elif r == c:
                                    cov[r * 6 + c] = 999
                        pose_w_cov_msg.covariance = cov
                        poses_msg.poses.append(pose_w_cov_msg)
                        print(pose_w_cov_msg.pose.position)
                    else:
                        rospy.logerr("Invalid distance estimate {}! Skipping detection.".format(estimated_distance)) 

                self._poses_pub.publish(poses_msg)
                   
                # visualize results
                if self._visualization:
                    v = Visualizer(np_image[:, :, ::-1], MetadataCatalog.get(self.cfg1.DATASETS.TRAIN[0]), scale=1.0)
                    v = v.draw_instance_predictions(detections)
                    img = v.get_image()[:, :, ::-1]

                    image_msg = self._bridge.cv2_to_imgmsg(img, 'bgr8')
                    self._vis_pub.publish(image_msg)

                # count processing time
                self._image_counter = self._image_counter + 1
                if (self._image_counter % 11) == 10:
                    rospy.loginfo("Images detected per second=%.2f",
                                  float(self._image_counter) / (time.time() - self.start_time))

            rate.sleep()


    def get_detections_msg(self, detections):

        boxes = detections.pred_boxes if detections.has("pred_boxes") else None

        dets_msg = Detections()
        dets_msg.header = self._header

        for i, (x1, y1, x2, y2) in enumerate(boxes):
            det_msg = Detection()

            det_msg.class_id = detections.pred_classes[i].numpy()
            det_msg.label = self._class_names[det_msg.class_id]
            det_msg.confidence = detections.scores[i].numpy()

            det_msg.xmin = np.uint32(x1)
            det_msg.ymin = np.uint32(y1)
            det_msg.xmax = np.uint32(x2)
            det_msg.ymax = np.uint32(y2)

            dets_msg.detections.append(det_msg)

        return dets_msg

    def convert_to_cv_image(self, image_msg):
        if image_msg is None:
            return None

        cv_img = self._bridge.imgmsg_to_cv2(image_msg)

        if image_msg.encoding.lower() == 'mono8':
            #cv_img = cv.cvtColor(cv_img, cv.COLOR_RGB2GRAY)
            cv_img = cv.cvtColor(cv_img, cv.COLOR_GRAY2BGR)  # TODO - quick fix for now
            cv_img = cv.normalize(cv_img, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)  # normalizing for thermal images

        return cv_img

    def rotation_matrix_from_vectors(self, vec1, vec2):
        """ Find the rotation matrix that aligns vec1 to vec2
        :param vec1: A 3d "source" vector
        :param vec2: A 3d "destination" vector
        :return mat: A transform matrix (3x3) which when applied to vec1, aligns it with vec2.
        """
        a, b = (vec1 / np.linalg.norm(vec1)).reshape(3), (vec2 / np.linalg.norm(vec2)).reshape(3)
        v = np.cross(a, b)
        c = np.dot(a, b)
        s = np.linalg.norm(v)
        kmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])
        rotation_matrix = np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2))
        return rotation_matrix

    #def generate_covariance(center_vector, right_vector): # TODO
        #return dets_msg

    def callback_image(self, msg):
        rospy.logdebug("Got an image")
        if self._msg_lock.acquire(False):
            self._last_msg = msg
            self._header = msg.header
            self._msg_lock.release()

    def callback_cam_info(self, msg):
        rospy.logdebug("Got camera info")
        if not self._camera_info:
            self._camera_info = msg
            print(self._camera_info)
            #self._camera_info.P = [1.0, 0.0, 0.0, 0.0,   # TODO cannot get this matrix from thermal camera for some reason
            #                        0.0, 1.0, 0.0, 0.0, 
            #                        0.0, 0.0, 0.0, 0.0]
            self.camera.fromCameraInfo(self._camera_info)

    @staticmethod
    def load_param(param, default=None):
        new_param = rospy.get_param(param, default)
        rospy.loginfo("[DroneLocalization] %s: %s", param, new_param)
        return new_param

def main(argv):
    rospy.init_node('drone_localization_ros')
    node = DroneLocalizationNode()
    node.run()

if __name__ == '__main__':
    main(sys.argv)
